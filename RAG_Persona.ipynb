{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OpenAI_API_Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마크다운 파일 경로\n",
    "markdown_path = \"prompt_engineering_tech.md\"\n",
    "\n",
    "# 마크다운 로더 초기화 및 데이터 로드\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "data = loader.load()\n",
    "\n",
    "# 마크다운 텍스트 스플리터 초기화 및 문서 분할\n",
    "markdown_splitter = MarkdownTextSplitter()\n",
    "markdown_split_docs = markdown_splitter.split_documents(data)\n",
    "\n",
    "# 재귀적 문자 텍스트 스플리터 초기화\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "char_split_docs = text_splitter.split_documents(markdown_split_docs)\n",
    "# Document 객체에서 텍스트 추출\n",
    "texts = [doc.page_content for doc in char_split_docs]\n",
    "# 임베딩 모델 초기화\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "# 텍스트 리스트를 embed_documents 메서드에 전달하여 임베딩 생성\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "# 벡터 스토어 초기화 및 임베딩 저장\n",
    "vector_store = FAISS.from_texts(texts, embedding_model)\n",
    "# 검색 모델 초기화\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페르소나 정의\n",
    "persona_description = '''\n",
    "From now on, You'll act an professional korean prompt engineer, 'Mr.P'. Mr.P is nerdy speaker and love teaching prompt engineering techniques to user.\n",
    "Mr.P always tried to talk about prompt engineering, User could feel he looks mad.\n",
    "'''\n",
    "RAG_Persona_template = f'''\n",
    "You should act an persona, and explain user's question based on context.\n",
    "{persona_description}\n",
    "\n",
    "When you get qusetions not related context, You don't need to try find answer about that. answer based on your knowledges.\n",
    "\n",
    "그리고 한글로 설명해주세요, 반드시!\n",
    "Answer the question based on the following context: {{context}}\n",
    "Question: {{question}}\n",
    "Answer:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 모델 초기화\n",
    "llm = OpenAI(api_key=api_key)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=RAG_Persona_template\n",
    ")\n",
    "# LLM 체인 초기화\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "# QA 체인 로드\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=qa_chain,\n",
    "    input_key=\"query\",\n",
    "    \n",
    ")\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "question = \"먼저 자기소개를 해주세요. 그리고 나서 CoT를 설명해줘.\"\n",
    "input_data = {\"query\": question}\n",
    "answer = rag_chain.run(input_data)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
